ViT(
  (positional_embedding): PositionalEmbedding2d()
  (encoder_blocks): Sequential(
    (0): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (1): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
  )
  (final_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=28, out_features=10, bias=True)
    )
  )
)
patches_train.shape: (60000, 49, 16)
learning rate: 0.019047619047619046
[grad] final_mlp.layers.0.weight
tensor([[-6.3725e-03, -1.3886e-03, -1.0045e-03,  4.1217e-04,  3.6772e-03,
          3.3778e-03, -4.5044e-03,  4.5137e-03, -4.8314e-03, -5.6380e-03,
          4.8968e-03, -1.5429e-02,  3.1933e-03, -2.9564e-03,  5.4361e-03,
          3.8789e-03,  5.7428e-03, -1.8530e-02,  3.4517e-03,  8.8761e-03,
         -2.7513e-03, -3.4858e-03, -9.8342e-03, -2.7796e-03,  3.0058e-03,
          4.7006e-03, -1.7933e-03,  1.7062e-02],
        [ 7.7470e-03, -7.6059e-04, -1.6302e-04, -4.8773e-03, -5.7315e-03,
          2.3415e-03,  6.5215e-03, -2.3485e-03,  2.6491e-03,  5.0202e-03,
         -4.7236e-04,  8.2845e-03, -6.8990e-03,  1.6082e-03, -8.9651e-03,
         -8.4905e-03, -4.9386e-03,  9.2472e-03, -2.4061e-03, -5.0534e-03,
          3.8463e-03,  2.5528e-03,  9.1213e-03,  2.2260e-03, -4.5043e-03,
         -1.3964e-03,  3.5609e-03, -1.0771e-02],
        [-2.7808e-03, -2.1133e-03, -1.7112e-03, -6.1561e-04,  1.8129e-03,
          1.4098e-03, -3.3632e-03,  5.3426e-04, -2.5488e-03, -3.0010e-03,
          2.2783e-03, -6.3358e-03,  1.6132e-03, -1.2464e-04, -4.8624e-05,
          1.2777e-03,  1.2138e-03, -7.1118e-03,  1.7671e-03,  2.0718e-03,
          1.4691e-03, -2.8597e-03, -3.6258e-03,  1.3569e-03,  9.1188e-04,
          2.3108e-03, -1.3001e-03,  4.6125e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00],
        [-3.4430e-04,  3.7755e-03,  1.1054e-03, -2.6315e-03,  2.9999e-03,
          1.0490e-02,  3.5558e-03, -4.0207e-03, -7.7029e-03, -3.2282e-03,
          5.5705e-03, -3.5434e-03,  1.6548e-03, -6.6374e-03, -1.6005e-03,
          1.5715e-03,  8.4106e-03, -4.6249e-03,  8.2009e-04,  1.9817e-03,
          2.8882e-03, -2.1220e-03, -6.0720e-03, -3.0048e-04, -2.6196e-03,
         -4.2290e-03, -3.5057e-03,  5.8513e-03],
        [ 1.2299e-03,  2.8846e-03, -3.4495e-04,  1.1919e-03,  6.2104e-04,
         -1.5650e-03,  1.0516e-03,  1.8265e-03, -9.0606e-04, -6.8342e-04,
          5.2108e-04, -7.3871e-04,  6.1372e-04, -1.6118e-03,  6.4403e-04,
         -6.7662e-05,  1.1174e-03, -7.4659e-04,  3.2865e-04,  1.9021e-03,
         -1.8296e-03,  1.9563e-03, -1.7198e-04, -2.0525e-03, -6.7884e-04,
         -3.6417e-04,  1.2741e-03,  1.4503e-03],
        [-7.6752e-04, -2.3647e-03,  3.5556e-03,  1.1288e-03, -1.8380e-03,
          5.2961e-04, -5.9881e-04, -4.3223e-03,  3.1613e-03,  3.0037e-03,
         -2.1089e-03,  5.8954e-03, -2.3634e-03,  7.4196e-04,  2.3243e-03,
          2.0193e-03, -1.9389e-03,  6.9691e-03,  2.8955e-04, -4.9838e-03,
         -1.9006e-04,  1.3117e-05,  1.3746e-03,  9.7410e-04,  1.0676e-03,
         -2.4401e-03, -1.9675e-03, -6.2844e-03]])
[grad] final_mlp.layers.0.bias
tensor([-4.6566e-09,  1.8626e-08, -9.3132e-09,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  4.0222e-03,  1.6298e-09, -3.9581e-09])
loss: [1]: 230.25497436523438
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
learning rate: 0.018140589569160995
[grad] final_mlp.layers.0.weight
tensor([[-6.3954e-03, -1.3918e-03, -1.0033e-03,  4.0434e-04,  3.7114e-03,
          3.3844e-03, -4.5136e-03,  4.5189e-03, -4.8367e-03, -5.6463e-03,
          4.9067e-03, -1.5436e-02,  3.1988e-03, -2.9698e-03,  5.4124e-03,
          3.8711e-03,  5.7419e-03, -1.8562e-02,  3.4197e-03,  8.8589e-03,
         -2.7846e-03, -3.4948e-03, -9.8381e-03, -2.7924e-03,  3.0356e-03,
          4.7212e-03, -1.7912e-03,  1.7042e-02],
        [ 7.7619e-03, -7.5823e-04, -1.6325e-04, -4.8735e-03, -5.7584e-03,
          2.3495e-03,  6.5302e-03, -2.3472e-03,  2.6533e-03,  5.0252e-03,
         -4.7594e-04,  8.2822e-03, -6.9126e-03,  1.6163e-03, -8.9510e-03,
         -8.4937e-03, -4.9348e-03,  9.2599e-03, -2.3801e-03, -5.0387e-03,
          3.8743e-03,  2.5636e-03,  9.1232e-03,  2.2405e-03, -4.5293e-03,
         -1.4117e-03,  3.5648e-03, -1.0751e-02],
        [-2.7873e-03, -2.1142e-03, -1.7119e-03, -6.1743e-04,  1.8255e-03,
          1.4078e-03, -3.3662e-03,  5.3565e-04, -2.5518e-03, -3.0030e-03,
          2.2820e-03, -6.3370e-03,  1.6187e-03, -1.2778e-04, -5.6112e-05,
          1.2767e-03,  1.2136e-03, -7.1193e-03,  1.7555e-03,  2.0658e-03,
          1.4570e-03, -2.8634e-03, -3.6259e-03,  1.3503e-03,  9.2199e-04,
          2.3183e-03, -1.3007e-03,  4.6047e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.6703e-04,  1.9576e-03,  5.0654e-04, -1.2602e-03,  1.5535e-03,
          5.2324e-03,  1.8421e-03, -1.9063e-03, -3.9076e-03, -1.7513e-03,
          2.8879e-03, -1.8954e-03,  8.4323e-04, -3.3027e-03, -7.9297e-04,
          7.2017e-04,  4.2823e-03, -2.4524e-03,  4.3491e-04,  1.0883e-03,
          1.4457e-03, -1.0177e-03, -3.0667e-03, -1.9630e-04, -1.3484e-03,
         -2.1091e-03, -1.7074e-03,  3.0500e-03],
        [ 1.2275e-03,  2.8843e-03, -3.4488e-04,  1.1908e-03,  6.2472e-04,
         -1.5651e-03,  1.0514e-03,  1.8274e-03, -9.0668e-04, -6.8433e-04,
          5.2049e-04, -7.3861e-04,  6.1453e-04, -1.6137e-03,  6.4166e-04,
         -6.9574e-05,  1.1169e-03, -7.4858e-04,  3.2478e-04,  1.8999e-03,
         -1.8322e-03,  1.9561e-03, -1.7149e-04, -2.0535e-03, -6.7559e-04,
         -3.6244e-04,  1.2745e-03,  1.4479e-03],
        [-7.6060e-04, -2.3643e-03,  3.5562e-03,  1.1306e-03, -1.8507e-03,
          5.2810e-04, -5.9651e-04, -4.3269e-03,  3.1636e-03,  3.0068e-03,
         -2.1116e-03,  5.8974e-03, -2.3663e-03,  7.4644e-04,  2.3307e-03,
          2.0241e-03, -1.9398e-03,  6.9781e-03,  3.0141e-04, -4.9773e-03,
         -1.8036e-04,  1.3870e-05,  1.3755e-03,  9.7919e-04,  1.0584e-03,
         -2.4459e-03, -1.9690e-03, -6.2788e-03]])
[grad] final_mlp.layers.0.bias
tensor([ 9.3132e-09,  1.1176e-08,  5.5879e-09,  0.0000e+00,  0.0000e+00,
         0.0000e+00,  0.0000e+00,  2.0072e-03,  0.0000e+00, -6.2864e-09])
loss: [2]: 230.2548370361328
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.28it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.81it/s]