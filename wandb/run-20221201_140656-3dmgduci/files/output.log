ViT(
  (positional_embedding): PositionalEmbedding2d()
  (encoder_blocks): Sequential(
    (0): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (1): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
  )
  (final_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=28, out_features=10, bias=True)
    )
  )
)
patches_train.shape: (60000, 49, 16)
learning rate: 0.019047619047619046
final_mlp.layers.0.weight
tensor([[ 2.4635e-02, -1.5254e-03, -4.8623e-03,  6.8082e-03,  3.3123e-03,
          9.7859e-03, -6.1533e-03, -2.2572e-02,  1.1529e-03, -1.4395e-02,
         -9.8283e-03,  1.0219e-02, -3.6737e-03, -1.5523e-02,  6.2532e-03,
         -7.5395e-03,  3.9255e-03, -1.0841e-02, -3.8712e-03,  2.2260e-03,
         -2.5825e-02,  3.9733e-03, -6.7463e-03,  4.5282e-03, -2.1594e-02,
         -1.8186e-02,  2.0774e-03,  3.8221e-03],
        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.2473e-04,  5.5302e-04, -1.4481e-03,  1.7885e-03, -1.1333e-04,
         -1.8581e-03, -1.4397e-04,  1.9914e-03,  1.8689e-03, -1.4482e-03,
         -4.0635e-04,  5.3122e-04, -2.4860e-04, -3.6734e-04, -2.7290e-03,
         -4.0804e-03,  5.1751e-04,  8.2648e-04, -2.1736e-03,  3.4748e-03,
          6.6356e-04,  1.2878e-03, -2.5633e-03,  2.5764e-03, -7.1418e-04,
          3.0814e-03,  3.8799e-03,  1.0863e-03],
        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.7044e-03,  9.4592e-04,  2.0846e-04, -1.1627e-03,  9.0664e-04,
          6.2630e-04,  1.5428e-03, -1.3984e-03, -4.7413e-04,  4.7966e-05,
          4.8358e-04, -1.4816e-03,  4.5698e-04, -4.8546e-04,  1.5274e-03,
          2.5967e-03, -7.9756e-05, -1.9990e-03,  2.3857e-03, -1.6953e-04,
          4.3045e-04,  2.6459e-03,  2.0586e-03, -1.0993e-03,  1.0429e-03,
         -1.8254e-03, -2.1640e-03, -1.0714e-03],
        [ 2.1107e-03, -1.6692e-03, -1.0159e-03,  1.0361e-03,  4.0399e-04,
          4.3226e-04, -6.9616e-04, -1.2818e-03, -7.3716e-04,  1.0297e-03,
         -8.0322e-05,  2.0263e-04, -1.0104e-03,  6.6925e-04,  7.5378e-05,
          2.9644e-05, -1.0933e-03,  6.6608e-04, -1.1455e-04, -1.1637e-03,
         -1.1313e-03, -1.9823e-04, -2.7253e-04, -2.7413e-04,  1.4241e-04,
         -1.2111e-03,  4.6023e-04,  3.0514e-05],
        [-9.1098e-04, -1.1191e-04,  4.6818e-03, -2.6191e-03,  1.9216e-03,
          2.0515e-03,  1.2694e-05, -2.9725e-03, -3.0479e-03,  1.3954e-03,
          7.9112e-04, -3.8675e-04,  8.7030e-04,  1.4640e-04,  1.9722e-03,
          5.0547e-03,  1.1150e-03, -1.1891e-03,  3.1453e-03, -3.0517e-03,
          3.4736e-04,  9.5682e-05,  3.1202e-03, -1.7095e-03,  1.0472e-03,
         -4.1501e-03, -4.5940e-03, -1.5306e-03],
        [ 5.2161e-04, -2.4961e-04, -4.8525e-03, -1.1484e-04, -1.5215e-04,
          3.8099e-04, -2.4484e-04,  6.9810e-04,  9.8200e-04,  1.3849e-04,
          1.0974e-03, -1.3063e-04,  8.6172e-04, -5.9214e-04, -1.0610e-03,
         -9.9594e-04, -2.0868e-04,  1.5875e-03, -1.5661e-03,  3.1261e-03,
         -7.5876e-04,  1.0114e-03, -2.4391e-03,  1.4307e-03, -1.8939e-03,
          1.6966e-03,  2.6209e-03, -3.2490e-04],
        [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00,  0.0000e+00]])
final_mlp.layers.0.bias
tensor([ 9.4862e-03,  0.0000e+00,  0.0000e+00,  1.4435e-08,  0.0000e+00,
        -9.3132e-09, -1.2573e-08,  1.8626e-09,  1.1874e-08,  0.0000e+00])
loss: [1]: 230.25717163085938
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
learning rate: 0.018140589569160995
final_mlp.layers.0.weight
tensor([[-1.5621e-02,  1.0744e-03,  2.3179e-03, -4.1911e-03, -1.6844e-03,
         -5.5262e-03,  2.8504e-03,  1.4607e-02, -3.3313e-04,  9.3379e-03,
          6.7130e-03, -5.9911e-03,  2.6325e-03,  9.9316e-03, -4.6278e-03,
          4.3877e-03, -2.2636e-03,  7.7940e-03,  1.5631e-03, -2.9596e-04,
          1.6159e-02, -2.8603e-03,  3.4340e-03, -2.3546e-03,  1.2965e-02,
          1.2620e-02, -1.2245e-03, -2.6856e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.2322e-04,  5.5579e-04, -1.4454e-03,  1.7910e-03, -1.1144e-04,
         -1.8587e-03, -1.4709e-04,  1.9940e-03,  1.8709e-03, -1.4490e-03,
         -4.0807e-04,  5.3294e-04, -2.5138e-04, -3.6865e-04, -2.7329e-03,
         -4.0859e-03,  5.2037e-04,  8.2539e-04, -2.1738e-03,  3.4763e-03,
          6.6657e-04,  1.2891e-03, -2.5612e-03,  2.5764e-03, -7.1313e-04,
          3.0836e-03,  3.8816e-03,  1.0853e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.7048e-03,  9.4373e-04,  2.0762e-04, -1.1632e-03,  9.0480e-04,
          6.2442e-04,  1.5450e-03, -1.3978e-03, -4.7359e-04,  4.7897e-05,
          4.8426e-04, -1.4845e-03,  4.5853e-04, -4.8533e-04,  1.5288e-03,
          2.5997e-03, -8.1740e-05, -1.9976e-03,  2.3852e-03, -1.7151e-04,
          4.2914e-04,  2.6460e-03,  2.0575e-03, -1.0999e-03,  1.0415e-03,
         -1.8271e-03, -2.1642e-03, -1.0701e-03],
        [ 2.1100e-03, -1.6688e-03, -1.0167e-03,  1.0359e-03,  4.0441e-04,
          4.3361e-04, -6.9619e-04, -1.2829e-03, -7.3834e-04,  1.0303e-03,
         -8.0012e-05,  2.0292e-04, -1.0104e-03,  6.6991e-04,  7.6867e-05,
          3.0737e-05, -1.0932e-03,  6.6542e-04, -1.1434e-04, -1.1632e-03,
         -1.1317e-03, -1.9863e-04, -2.7237e-04, -2.7381e-04,  1.4293e-04,
         -1.2108e-03,  4.5977e-04,  3.0156e-05],
        [-9.1197e-04, -1.1584e-04,  4.6785e-03, -2.6209e-03,  1.9182e-03,
          2.0495e-03,  1.6161e-05, -2.9727e-03, -3.0484e-03,  1.3952e-03,
          7.9326e-04, -3.8992e-04,  8.7334e-04,  1.4788e-04,  1.9757e-03,
          5.0597e-03,  1.1111e-03, -1.1866e-03,  3.1431e-03, -3.0541e-03,
          3.4425e-04,  9.4243e-05,  3.1167e-03, -1.7100e-03,  1.0446e-03,
         -4.1526e-03, -4.5941e-03, -1.5276e-03],
        [ 5.2257e-04, -2.4805e-04, -4.8516e-03, -1.1364e-04, -1.5032e-04,
          3.8158e-04, -2.4705e-04,  6.9837e-04,  9.8247e-04,  1.3889e-04,
          1.0965e-03, -1.3016e-04,  8.5979e-04, -5.9290e-04, -1.0622e-03,
         -9.9805e-04, -2.0643e-04,  1.5870e-03, -1.5653e-03,  3.1276e-03,
         -7.5755e-04,  1.0123e-03, -2.4372e-03,  1.4311e-03, -1.8930e-03,
          1.6980e-03,  2.6209e-03, -3.2600e-04],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00]])
final_mlp.layers.0.bias
tensor([-6.1220e-03,  0.0000e+00,  0.0000e+00, -5.1223e-09,  0.0000e+00,
        -1.1176e-08,  1.3970e-09, -1.8626e-09,  3.0268e-09,  0.0000e+00])
loss: [2]: 230.25714111328125
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.81it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.19it/s]