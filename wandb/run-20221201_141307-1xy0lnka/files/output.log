ViT(
  (positional_embedding): PositionalEmbedding2d()
  (encoder_blocks): Sequential(
    (0): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (1): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
  )
  (final_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=28, out_features=10, bias=True)
    )
  )
)
patches_train.shape: (60000, 49, 16)
learning rate: 0.019047619047619046
[grad] final_mlp.layers.0.weight
tensor([[-1.2708e-03, -1.0770e-02,  2.4493e-03,  1.3504e-03, -1.1073e-03,
         -1.2474e-02,  6.3245e-03,  2.4169e-03,  7.9152e-03, -4.5102e-03,
         -3.4110e-03, -1.0122e-02,  8.6430e-03,  2.4520e-03,  2.7312e-03,
         -6.4242e-03, -2.3164e-03,  4.2906e-03, -1.2998e-03, -2.4191e-03,
          3.2032e-04, -3.9206e-03,  5.5973e-03,  4.4789e-03, -2.7540e-03,
         -1.1370e-03, -8.7404e-04,  6.2288e-03],
        [ 1.2056e-03,  6.9012e-03, -3.7602e-03, -8.4117e-04,  1.7259e-03,
          7.1763e-03, -2.6158e-03, -2.0144e-03, -4.6825e-03, -1.2482e-03,
          6.6570e-03,  1.0423e-02, -7.6698e-03, -3.8822e-03, -2.9258e-03,
          3.4153e-03, -7.1746e-06, -2.3067e-03,  2.1083e-03,  1.1890e-03,
          5.3651e-05,  4.3130e-03, -5.0665e-03, -7.0149e-04,  6.6300e-04,
          1.1725e-03,  1.2274e-03, -3.7235e-03],
        [ 3.2617e-04, -3.8038e-03,  5.0705e-04,  1.0458e-04, -4.0813e-04,
         -4.0854e-03,  2.4310e-03,  1.0001e-03,  2.7703e-03, -3.0256e-03,
          5.5964e-05, -1.9072e-03,  2.7655e-03, -2.7453e-04,  7.3025e-04,
         -2.5268e-03, -1.9158e-03,  1.6579e-03,  9.2445e-04, -1.5446e-03,
          4.1903e-04,  4.7767e-04,  1.0985e-03,  2.0497e-03, -2.1756e-03,
         -3.2313e-04,  1.9213e-04,  2.5044e-03],
        [-5.5364e-04, -1.4188e-03,  1.8331e-03,  4.4091e-04, -1.7056e-03,
         -1.3423e-03, -7.7140e-04,  3.7304e-04, -3.3978e-04,  8.6295e-04,
         -2.7952e-04, -4.5568e-03,  4.5113e-03,  1.3021e-03, -7.3022e-04,
         -1.2903e-03,  3.0927e-05,  1.5267e-04,  1.9065e-03,  6.9130e-04,
         -1.4189e-03, -1.2541e-03,  1.4328e-03, -1.6813e-04,  3.2093e-04,
          1.0761e-03, -1.3315e-03,  1.0702e-03],
        [-1.4284e-03,  2.6630e-03, -3.7698e-05,  1.3086e-03,  4.3163e-04,
          3.7329e-03, -1.4985e-03, -7.2923e-04, -3.1869e-03,  3.4604e-03,
          3.3193e-03, -6.9706e-04, -1.1538e-03, -5.1375e-04, -9.1241e-04,
          2.0900e-03, -9.8570e-04, -9.5114e-04, -9.3642e-04,  1.1747e-03,
          4.3907e-04, -3.8306e-04,  3.4583e-05, -6.5289e-04,  4.0418e-03,
          2.6652e-03, -7.6989e-04, -1.1564e-03],
        [-6.6548e-06,  2.3462e-03, -1.1292e-04, -6.8973e-04, -8.2143e-05,
          3.5700e-03, -2.7126e-03, -9.5984e-08, -2.2718e-03,  1.8406e-03,
          1.1689e-04,  1.1633e-03, -4.3705e-04, -2.9623e-05, -5.9179e-04,
          1.6252e-03,  9.4805e-04, -1.5199e-03,  3.2113e-04,  1.6398e-03,
         -1.1347e-03,  1.6119e-04, -2.2633e-04, -1.4226e-03,  7.9926e-04,
          2.4509e-04, -4.8392e-04, -1.5773e-03],
        [ 1.8189e-04, -1.1630e-03,  4.0064e-04, -2.6817e-04,  2.0751e-04,
         -1.9123e-03, -1.9494e-04,  4.0726e-04,  2.2151e-04,  8.8994e-04,
         -9.1518e-05, -3.3032e-04, -2.2788e-04,  1.1034e-03,  2.6539e-04,
          9.7759e-04, -2.3189e-04,  3.9976e-04, -4.2475e-04, -2.7101e-04,
          1.0860e-03, -8.1238e-04, -1.0336e-03, -6.6328e-04, -9.3377e-04,
          2.6024e-04,  8.8012e-04,  2.2925e-04],
        [ 1.9661e-03,  2.5263e-03, -2.6800e-03, -4.7114e-04,  2.1028e-03,
          4.1926e-03,  3.2384e-04, -1.0199e-03,  7.3507e-04, -1.5768e-04,
         -2.2267e-03,  5.0713e-03, -6.1859e-03, -2.0750e-03,  1.3710e-03,
          1.4194e-03,  1.4225e-03,  4.5289e-04, -2.5323e-03, -1.4633e-03,
          5.6438e-04,  1.6711e-03, -9.8067e-04, -2.3126e-04,  6.2111e-04,
         -2.1364e-03,  4.5041e-04, -1.3761e-03],
        [-1.9902e-03, -1.7253e-04,  2.5863e-03, -1.8427e-04, -1.1722e-03,
         -1.0364e-03, -1.5781e-04, -1.3399e-04, -5.4334e-04,  8.8995e-04,
         -1.8852e-03, -2.6645e-03,  3.5613e-03,  2.5196e-03, -2.4879e-04,
         -9.8200e-04,  8.5300e-04, -2.7423e-04,  8.7325e-04,  1.1776e-03,
          6.5578e-06, -1.0574e-03,  1.0197e-03, -1.2213e-03, -7.5882e-04,
         -2.2230e-04, -1.3385e-03,  5.6358e-04],
        [ 1.3557e-03,  3.0297e-03, -9.7028e-04, -7.5811e-04, -1.0795e-04,
          2.2137e-03, -1.2216e-03, -2.9679e-04, -9.7200e-04,  1.0162e-03,
         -1.8204e-03,  3.4671e-03, -3.4675e-03, -5.0040e-04,  5.1024e-05,
          1.6736e-03,  2.0532e-03, -2.0310e-03, -6.8583e-04,  4.5242e-05,
         -2.8828e-04,  9.2866e-04, -1.9396e-03, -1.4906e-03,  2.0950e-04,
         -1.3469e-03,  1.8871e-03, -2.7020e-03]])
[grad] final_mlp.layers.0.bias
tensor([-9.3132e-09, -9.3132e-10,  7.4506e-09, -9.3132e-10, -1.8626e-09,
         5.5879e-09,  2.7940e-09,  2.7940e-09,  2.5611e-09,  1.3970e-09])
loss: [1]: 230.2568817138672
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
learning rate: 0.018140589569160995
[grad] final_mlp.layers.0.weight
tensor([[-1.2552e-03, -1.0775e-02,  2.4671e-03,  1.3135e-03, -1.1434e-03,
         -1.2483e-02,  6.3160e-03,  2.4218e-03,  7.9062e-03, -4.4852e-03,
         -3.4595e-03, -1.0184e-02,  8.6515e-03,  2.4773e-03,  2.6935e-03,
         -6.4379e-03, -2.2938e-03,  4.2832e-03, -1.3205e-03, -2.4451e-03,
          3.2026e-04, -3.9496e-03,  5.6007e-03,  4.4507e-03, -2.6758e-03,
         -1.1348e-03, -9.1613e-04,  6.2459e-03],
        [ 1.1954e-03,  6.9016e-03, -3.7782e-03, -8.1013e-04,  1.7560e-03,
          7.1815e-03, -2.6059e-03, -2.0185e-03, -4.6702e-03, -1.2718e-03,
          6.6966e-03,  1.0468e-02, -7.6788e-03, -3.9049e-03, -2.8948e-03,
          3.4260e-03, -2.1864e-05, -2.2965e-03,  2.1213e-03,  1.2042e-03,
          5.9108e-05,  4.3368e-03, -5.0675e-03, -6.7699e-04,  6.0425e-04,
          1.1748e-03,  1.2571e-03, -3.7324e-03],
        [ 3.3017e-04, -3.8039e-03,  5.1393e-04,  9.1793e-05, -4.1931e-04,
         -4.0861e-03,  2.4268e-03,  1.0034e-03,  2.7666e-03, -3.0170e-03,
          4.3082e-05, -1.9263e-03,  2.7689e-03, -2.6739e-04,  7.1770e-04,
         -2.5296e-03, -1.9092e-03,  1.6540e-03,  9.1936e-04, -1.5524e-03,
          4.2065e-04,  4.6979e-04,  1.0992e-03,  2.0397e-03, -2.1520e-03,
         -3.2152e-04,  1.8098e-04,  2.5101e-03],
        [-5.5178e-04, -1.4192e-03,  1.8378e-03,  4.3284e-04, -1.7149e-03,
         -1.3440e-03, -7.7326e-04,  3.7445e-04, -3.4240e-04,  8.6885e-04,
         -2.8950e-04, -4.5704e-03,  4.5145e-03,  1.3086e-03, -7.3886e-04,
         -1.2926e-03,  3.4130e-05,  1.5110e-04,  1.9045e-03,  6.8596e-04,
         -1.4195e-03, -1.2611e-03,  1.4317e-03, -1.7487e-04,  3.3664e-04,
          1.0766e-03, -1.3392e-03,  1.0745e-03],
        [-1.4321e-03,  2.6634e-03, -4.0266e-05,  1.3158e-03,  4.3817e-04,
          3.7337e-03, -1.4965e-03, -7.3010e-04, -3.1849e-03,  3.4571e-03,
          3.3283e-03, -6.8860e-04, -1.1539e-03, -5.1757e-04, -9.0507e-04,
          2.0922e-03, -9.9002e-04, -9.4963e-04, -9.3224e-04,  1.1782e-03,
          4.3887e-04, -3.7895e-04,  3.3563e-05, -6.4867e-04,  4.0285e-03,
          2.6657e-03, -7.6447e-04, -1.1585e-03],
        [-9.9409e-06,  2.3465e-03, -1.1481e-04, -6.8338e-04, -7.7052e-05,
          3.5706e-03, -2.7105e-03, -9.1684e-07, -2.2704e-03,  1.8366e-03,
          1.2499e-04,  1.1737e-03, -4.3725e-04, -3.2861e-05, -5.8549e-04,
          1.6272e-03,  9.4306e-04, -1.5187e-03,  3.2577e-04,  1.6453e-03,
         -1.1357e-03,  1.6618e-04, -2.2761e-04, -1.4177e-03,  7.8474e-04,
          2.4350e-04, -4.7573e-04, -1.5805e-03],
        [ 1.8298e-04, -1.1632e-03,  4.0047e-04, -2.6945e-04,  2.0684e-04,
         -1.9132e-03, -1.9632e-04,  4.0704e-04,  2.1985e-04,  8.9061e-04,
         -9.3316e-05, -3.3087e-04, -2.2840e-04,  1.1038e-03,  2.6374e-04,
          9.7689e-04, -2.3073e-04,  3.9966e-04, -4.2586e-04, -2.7112e-04,
          1.0858e-03, -8.1408e-04, -1.0323e-03, -6.6303e-04, -9.3022e-04,
          2.6100e-04,  8.7697e-04,  2.2831e-04],
        [ 1.9622e-03,  2.5274e-03, -2.6841e-03, -4.6163e-04,  2.1126e-03,
          4.1942e-03,  3.2689e-04, -1.0215e-03,  7.3796e-04, -1.6386e-04,
         -2.2141e-03,  5.0885e-03, -6.1886e-03, -2.0818e-03,  1.3805e-03,
          1.4219e-03,  1.4158e-03,  4.5402e-04, -2.5290e-03, -1.4559e-03,
          5.6410e-04,  1.6796e-03, -9.8056e-04, -2.2367e-04,  6.0018e-04,
         -2.1390e-03,  4.6378e-04, -1.3814e-03],
        [-1.9881e-03, -1.7312e-04,  2.5890e-03, -1.8973e-04, -1.1784e-03,
         -1.0384e-03, -1.5971e-04, -1.3398e-04, -5.4570e-04,  8.9343e-04,
         -1.8934e-03, -2.6722e-03,  3.5625e-03,  2.5236e-03, -2.5409e-04,
         -9.8503e-04,  8.5537e-04, -2.7502e-04,  8.7155e-04,  1.1749e-03,
          4.0862e-06, -1.0627e-03,  1.0203e-03, -1.2257e-03, -7.4783e-04,
         -2.2305e-04, -1.3452e-03,  5.6498e-04],
        [ 1.3514e-03,  3.0295e-03, -9.7486e-04, -7.4636e-04, -9.7101e-05,
          2.2132e-03, -1.2173e-03, -2.9821e-04, -9.6820e-04,  1.0068e-03,
         -1.8063e-03,  3.4857e-03, -3.4683e-03, -5.0682e-04,  6.2643e-05,
          1.6760e-03,  2.0454e-03, -2.0294e-03, -6.7998e-04,  5.3905e-05,
         -2.8900e-04,  9.3727e-04, -1.9401e-03, -1.4798e-03,  1.8403e-04,
         -1.3487e-03,  1.9013e-03, -2.7071e-03]])
[grad] final_mlp.layers.0.bias
tensor([-1.4901e-08,  1.2107e-08, -1.5832e-08, -6.5193e-09, -1.8626e-09,
        -1.1176e-08,  4.6566e-10, -6.5193e-09,  4.4238e-09, -7.2177e-09])
loss: [2]: 230.2567138671875
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.72it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 39.25it/s]