ViT(
  (positional_embedding): PositionalEmbedding2d()
  (encoder_blocks): Sequential(
    (0): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (1): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
  )
  (final_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=28, out_features=10, bias=True)
    )
  )
)
patches_train.shape: (60000, 49, 16)
learning rate: 0.019047619047619046
[grad] final_mlp.layers.0.weight
tensor([[-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 7.1661e-03,  1.2201e-02, -1.1659e-03,  8.6083e-03, -4.5245e-03,
         -7.2217e-03, -3.8713e-03, -2.0965e-03, -7.8338e-03, -7.1748e-03,
         -6.2832e-03, -9.3893e-03, -4.9051e-03, -5.7113e-03,  4.6151e-03,
          5.8072e-04,  6.2045e-04, -5.3790e-03,  6.5296e-03,  1.1281e-02,
          5.2781e-03,  2.9641e-03, -2.1623e-03,  6.4281e-03, -9.0820e-03,
         -3.1639e-03,  2.3449e-03,  8.5002e-03],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 4.1022e-03,  1.1392e-04,  8.1557e-04,  1.6947e-03, -4.3585e-04,
          8.7705e-04,  7.0182e-04, -1.8941e-03,  4.2610e-04, -2.7058e-03,
          2.1165e-03, -2.3376e-03, -1.4342e-03, -1.8356e-03,  2.7547e-03,
          1.9760e-03,  4.2609e-04,  4.1916e-04,  1.9002e-03,  2.0793e-03,
         -1.1862e-05, -1.7494e-04, -2.5066e-03, -1.4776e-03, -7.1913e-04,
         -1.9083e-03, -1.3976e-03, -1.3185e-03],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00]])
[grad] final_mlp.layers.0.bias
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -6.6760e-03,  0.0000e+00,
        -1.1176e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])
loss: [1]: 230.25872802734375
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
learning rate: 0.018140589569160995
[grad] final_mlp.layers.0.weight
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 5.8340e-03,  1.0276e-02, -1.0149e-03,  7.1485e-03, -3.8523e-03,
         -6.1368e-03, -3.2658e-03, -1.7646e-03, -6.6094e-03, -6.0062e-03,
         -5.4581e-03, -7.8856e-03, -4.1329e-03, -4.6125e-03,  3.8008e-03,
          4.2685e-04,  5.6326e-04, -4.5521e-03,  5.4467e-03,  9.3204e-03,
          4.3437e-03,  2.4071e-03, -1.8651e-03,  5.4596e-03, -7.5862e-03,
         -2.5661e-03,  2.0438e-03,  7.2536e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 4.1008e-03,  1.1562e-04,  8.1412e-04,  1.6961e-03, -4.3613e-04,
          8.7741e-04,  7.0355e-04, -1.8945e-03,  4.2656e-04, -2.7057e-03,
          2.1149e-03, -2.3375e-03, -1.4350e-03, -1.8333e-03,  2.7556e-03,
          1.9746e-03,  4.2797e-04,  4.2009e-04,  1.8998e-03,  2.0802e-03,
         -1.3456e-05, -1.7446e-04, -2.5068e-03, -1.4789e-03, -7.1665e-04,
         -1.9069e-03, -1.3965e-03, -1.3168e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00]])
[grad] final_mlp.layers.0.bias
tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -5.6066e-03,  0.0000e+00,
        -1.4901e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])
loss: [2]: 230.2587127685547
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.79it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 31.24it/s]