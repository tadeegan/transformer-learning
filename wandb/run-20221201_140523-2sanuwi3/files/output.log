ViT(
  (positional_embedding): PositionalEmbedding2d()
  (encoder_blocks): Sequential(
    (0): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (1): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
  )
  (final_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=28, out_features=10, bias=True)
    )
  )
)
patches_train.shape: (60000, 49, 16)
learning rate: 0.019047619047619046
final_mlp.layers.0.weight
tensor([[-1.9139e-04, -1.5514e-02,  4.6639e-03, -5.2582e-03,  5.3619e-03,
          4.9446e-03,  7.4238e-03, -1.1611e-02, -7.7357e-03, -5.7571e-03,
         -4.1775e-03, -1.3134e-02,  2.2750e-04,  1.3870e-02, -5.6557e-05,
          8.5998e-03,  7.6002e-04,  1.3333e-02,  4.8717e-03, -1.3101e-02,
         -1.1583e-02,  5.2767e-03,  2.9144e-03,  8.3788e-03, -6.8888e-03,
         -1.1334e-02,  5.0423e-03, -3.9797e-03],
        [ 2.1558e-03,  1.1103e-02, -3.5502e-03,  3.1415e-03, -5.9342e-03,
         -2.7246e-03, -5.4476e-03,  6.6232e-03,  6.3541e-03,  3.3359e-03,
          4.7845e-03,  1.2010e-02, -8.8446e-05, -1.0257e-02, -4.0788e-04,
         -7.9552e-03, -1.8969e-03, -1.0820e-02, -1.4613e-03,  1.0733e-02,
          1.0673e-02, -2.1780e-03, -1.3884e-03, -6.7826e-03,  6.5356e-03,
          8.5498e-03, -3.1706e-03,  2.8141e-03],
        [ 1.8570e-02,  1.0014e-02, -1.7393e-02,  2.0899e-02,  1.0073e-02,
          3.7977e-02, -3.5658e-02, -1.3082e-02,  3.8797e-02,  6.8690e-02,
          2.8515e-04, -4.3553e-02,  6.1143e-02, -7.1151e-02, -2.5370e-02,
          7.5878e-03,  4.3919e-03, -5.5031e-03,  5.0006e-02,  5.3402e-02,
          2.0025e-02,  1.4374e-02, -6.4474e-03, -5.1835e-02, -7.9558e-03,
         -3.7903e-02, -4.5057e-02, -4.1877e-02],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.3649e-05,  2.8373e-03, -1.6362e-03,  2.6356e-04, -5.9746e-04,
         -5.6296e-04, -3.3841e-03,  8.4276e-04,  2.8177e-03,  2.2194e-03,
         -4.6210e-04,  3.2237e-03, -1.0212e-03, -3.7802e-03, -1.5449e-03,
         -2.0582e-03,  1.1801e-03, -1.0872e-03,  4.6676e-04,  2.4703e-03,
          7.2446e-04, -2.0335e-03, -1.0618e-03, -2.7956e-03,  1.2131e-03,
          1.1807e-03, -3.9501e-04,  8.9128e-04],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.7806e-03, -8.9793e-04,  9.2424e-04,  1.2467e-03, -4.5477e-05,
         -2.5373e-04,  5.6317e-04,  1.9144e-03, -1.4741e-03, -1.1810e-04,
         -5.0310e-04, -1.2278e-03, -5.9376e-04,  1.1394e-03, -5.6395e-04,
          1.2163e-03, -2.9168e-04,  2.5901e-05, -4.5223e-04, -1.0823e-03,
         -8.5900e-04,  3.8982e-04, -1.4910e-03,  1.0060e-03, -3.6970e-04,
         -7.9208e-04, -1.2524e-03,  1.1366e-03],
        [ 1.3710e-03,  6.4214e-03, -1.4741e-03,  1.5189e-03, -1.3987e-03,
         -2.1357e-03, -1.9864e-03,  5.1098e-03,  1.1332e-03,  1.0809e-03,
          2.6739e-03,  3.2322e-03,  2.7816e-04, -4.7756e-03,  1.9007e-03,
         -1.3128e-03, -2.2609e-03, -5.1341e-03, -2.3221e-03,  2.5631e-03,
          4.3416e-03, -2.7109e-03, -9.9916e-04, -2.4214e-03,  1.6156e-03,
          3.7659e-03, -1.2497e-03,  3.1776e-05],
        [-1.0833e-03, -2.0909e-03, -3.2642e-05, -2.0860e-04,  1.8401e-03,
          1.2516e-03,  1.7700e-04, -8.5626e-04, -4.3525e-04,  5.1058e-04,
         -2.3437e-03, -2.5023e-03,  5.8053e-04,  1.9114e-03, -7.9574e-04,
          6.3452e-04,  1.2412e-03,  2.2041e-03, -5.0512e-04, -2.6922e-03,
         -3.7560e-03,  5.0886e-04,  2.7092e-04,  1.2840e-03, -1.8938e-03,
         -1.3731e-03,  4.7288e-04, -9.0443e-05],
        [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00]])
final_mlp.layers.0.bias
tensor([ 3.7253e-09,  1.6764e-08, -3.2303e-02,  0.0000e+00, -7.4506e-09,
         0.0000e+00,  4.6566e-10,  4.6566e-09,  6.2864e-09,  0.0000e+00])
loss: [1]: 230.26571655273438
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
learning rate: 0.018140589569160995
final_mlp.layers.0.weight
tensor([[-1.8641e-04, -1.5517e-02,  4.6195e-03, -5.2363e-03,  5.3983e-03,
          4.9402e-03,  7.4440e-03, -1.1612e-02, -7.7272e-03, -5.7616e-03,
         -4.1503e-03, -1.3168e-02,  2.4361e-04,  1.3883e-02, -7.1111e-05,
          8.6030e-03,  7.7764e-04,  1.3326e-02,  4.8516e-03, -1.3068e-02,
         -1.1577e-02,  5.2769e-03,  2.9171e-03,  8.3577e-03, -6.9276e-03,
         -1.1282e-02,  5.0428e-03, -3.9438e-03],
        [ 2.1527e-03,  1.1097e-02, -3.5155e-03,  3.1249e-03, -5.9621e-03,
         -2.7148e-03, -5.4621e-03,  6.6165e-03,  6.3425e-03,  3.3405e-03,
          4.7596e-03,  1.2034e-02, -1.0162e-04, -1.0262e-02, -4.0129e-04,
         -7.9563e-03, -1.9132e-03, -1.0806e-02, -1.4388e-03,  1.0699e-02,
          1.0664e-02, -2.1729e-03, -1.3942e-03, -6.7650e-03,  6.5675e-03,
          8.5006e-03, -3.1688e-03,  2.7831e-03],
        [ 1.2936e-02,  4.8480e-03, -1.1186e-02,  1.3038e-02,  7.0276e-03,
          2.7137e-02, -2.3582e-02, -1.0523e-02,  2.5745e-02,  4.6292e-02,
         -2.8152e-05, -3.1240e-02,  4.2075e-02, -4.7241e-02, -1.7641e-02,
          5.7516e-03,  2.9337e-03, -2.6003e-03,  3.5429e-02,  3.6333e-02,
          1.3435e-02,  1.1307e-02, -4.3368e-03, -3.4676e-02, -5.6833e-03,
         -2.7429e-02, -3.0325e-02, -2.9016e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.4867e-05,  2.8348e-03, -1.6262e-03,  2.5935e-04, -6.0325e-04,
         -5.6284e-04, -3.3862e-03,  8.4264e-04,  2.8150e-03,  2.2181e-03,
         -4.6615e-04,  3.2283e-03, -1.0221e-03, -3.7811e-03, -1.5418e-03,
         -2.0594e-03,  1.1777e-03, -1.0847e-03,  4.7054e-04,  2.4631e-03,
          7.2308e-04, -2.0306e-03, -1.0593e-03, -2.7907e-03,  1.2187e-03,
          1.1696e-03, -3.9429e-04,  8.8247e-04],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.7797e-03, -8.9794e-04,  9.2225e-04,  1.2464e-03, -4.5109e-05,
         -2.5340e-04,  5.6400e-04,  1.9142e-03, -1.4735e-03, -1.1758e-04,
         -5.0208e-04, -1.2279e-03, -5.9291e-04,  1.1397e-03, -5.6404e-04,
          1.2167e-03, -2.9138e-04,  2.4697e-05, -4.5327e-04, -1.0807e-03,
         -8.5872e-04,  3.8898e-04, -1.4912e-03,  1.0039e-03, -3.7136e-04,
         -7.8921e-04, -1.2526e-03,  1.1381e-03],
        [ 1.3706e-03,  6.4197e-03, -1.4591e-03,  1.5132e-03, -1.4091e-03,
         -2.1343e-03, -1.9924e-03,  5.1077e-03,  1.1300e-03,  1.0824e-03,
          2.6651e-03,  3.2391e-03,  2.7488e-04, -4.7767e-03,  1.9047e-03,
         -1.3134e-03, -2.2626e-03, -5.1291e-03, -2.3155e-03,  2.5508e-03,
          4.3361e-03, -2.7114e-03, -9.9757e-04, -2.4143e-03,  1.6242e-03,
          3.7508e-03, -1.2493e-03,  1.9013e-05],
        [-1.0836e-03, -2.0911e-03, -4.0016e-05, -2.0762e-04,  1.8454e-03,
          1.2512e-03,  1.8133e-04, -8.5657e-04, -4.3296e-04,  5.0845e-04,
         -2.3387e-03, -2.5045e-03,  5.8037e-04,  1.9110e-03, -7.9726e-04,
          6.3658e-04,  1.2399e-03,  2.2015e-03, -5.0768e-04, -2.6854e-03,
         -3.7527e-03,  5.0991e-04,  2.6895e-04,  1.2821e-03, -1.8966e-03,
         -1.3666e-03,  4.7250e-04, -8.3482e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00]])
final_mlp.layers.0.bias
tensor([-3.2596e-09, -6.5193e-09, -2.2340e-02,  0.0000e+00, -3.7253e-09,
         0.0000e+00,  2.7940e-09, -8.3819e-09, -1.6997e-08,  0.0000e+00])
loss: [2]: 230.26449584960938
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.33it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.66it/s]