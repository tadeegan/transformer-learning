ViT(
  (positional_embedding): PositionalEmbedding2d()
  (encoder_blocks): Sequential(
    (0): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (1): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
  )
  (final_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=28, out_features=10, bias=True)
    )
  )
)
patches_train.shape: (60000, 49, 16)
learning rate: 0.019047619047619046
[grad] final_mlp.layers.0.weight
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-2.2000e-03,  7.0041e-03,  1.2220e-03,  7.2441e-03, -3.9895e-03,
          2.0090e-03,  1.2081e-03, -2.7863e-03, -6.6961e-04, -1.8847e-03,
         -8.0314e-03, -1.8116e-03, -3.7218e-03, -1.3788e-02,  2.9780e-03,
         -4.2339e-03, -4.1878e-03, -4.5958e-03, -9.2862e-04, -5.4899e-03,
         -1.0192e-03, -1.9246e-03, -3.1932e-04,  2.2434e-04, -1.0694e-02,
         -5.3599e-03, -2.1343e-03,  4.2100e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 1.4357e-03, -2.5568e-03, -8.5319e-04, -2.5231e-03,  1.7694e-03,
         -3.1277e-03,  3.8553e-05, -6.6774e-04,  1.6390e-04,  2.0913e-03,
         -7.4719e-04, -3.7802e-04, -2.3602e-03,  1.4906e-03, -1.4937e-03,
          5.8219e-04,  1.2269e-03,  1.2648e-03,  3.8850e-04, -1.4637e-03,
         -2.4109e-04,  1.6305e-04, -6.6168e-04, -4.9983e-04,  3.2061e-03,
          9.9619e-04,  5.7328e-04,  9.3854e-04],
        [ 6.8324e-04,  9.5300e-04, -9.9519e-04,  3.3361e-03, -8.5298e-04,
          2.1604e-03,  2.0196e-03,  6.6323e-05,  6.5204e-04, -6.4092e-04,
         -2.9776e-03, -2.3674e-03, -7.2566e-04, -2.8353e-03,  1.2303e-03,
          1.2869e-03,  6.6903e-04, -2.2765e-03, -2.5555e-03, -1.2184e-03,
          4.0805e-03, -9.8188e-04, -2.6271e-03, -1.4592e-03, -2.5536e-04,
         -6.4588e-04,  1.6758e-04,  2.7021e-03],
        [ 5.4151e-04,  1.6627e-04,  1.3665e-04,  1.0982e-03, -2.0393e-03,
          2.1589e-04, -1.6454e-03, -2.5370e-04, -2.3894e-04, -7.7201e-04,
         -1.6726e-03, -1.0077e-03,  8.3255e-04, -2.5088e-03,  9.8938e-04,
         -1.2829e-03,  1.0002e-03,  1.8250e-05, -1.1791e-03, -1.0292e-03,
          9.2247e-04, -3.2726e-04,  2.9742e-04, -1.0152e-03, -2.0309e-03,
         -1.3798e-04, -1.4086e-03,  3.3047e-04],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-1.6778e-03,  1.0792e-03,  1.5621e-03,  1.3312e-03, -3.0026e-03,
          2.6127e-03, -1.0837e-03,  1.4091e-03,  1.7325e-04, -2.2362e-03,
         -4.6204e-04, -7.4389e-04,  2.9329e-03, -2.2503e-03,  3.3778e-03,
         -1.8751e-03, -7.8784e-05,  4.8808e-04, -5.4125e-04,  5.9372e-04,
         -4.9656e-04,  6.4004e-04,  1.2294e-03, -8.5871e-04, -2.4928e-03,
         -4.2416e-04, -1.6362e-03, -1.6148e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,
         -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
          0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-4.5809e-04,  1.3467e-03,  1.3395e-03,  9.1277e-04, -3.2496e-03,
          1.2874e-03, -2.0024e-03, -3.2242e-04, -7.3572e-04, -2.8970e-03,
         -7.4538e-04, -1.1548e-03,  1.7110e-03, -2.0578e-03,  2.6822e-03,
         -1.7789e-03,  8.8881e-05, -1.6475e-04, -8.3840e-04, -6.4940e-04,
         -1.1600e-03,  7.6493e-04,  1.2060e-03,  2.2229e-04, -3.4973e-03,
         -9.7249e-04, -9.2093e-04, -5.9505e-04]])
[grad] final_mlp.layers.0.bias
tensor([ 0.0000e+00, -1.7695e-08,  0.0000e+00, -5.5879e-09, -1.8626e-09,
         0.0000e+00,  0.0000e+00,  9.3132e-10,  0.0000e+00, -7.9162e-09])
loss: [1]: 230.2572021484375
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
learning rate: 0.018140589569160995
[grad] final_mlp.layers.0.weight
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.1969e-03,  7.0037e-03,  1.2375e-03,  7.2457e-03, -3.9901e-03,
          2.0069e-03,  1.1994e-03, -2.7837e-03, -6.5004e-04, -1.8810e-03,
         -8.0416e-03, -1.8126e-03, -3.7178e-03, -1.3769e-02,  2.9915e-03,
         -4.2490e-03, -4.1889e-03, -4.5950e-03, -9.3526e-04, -5.5005e-03,
         -1.0299e-03, -1.9163e-03, -3.1082e-04,  2.3379e-04, -1.0693e-02,
         -5.3624e-03, -2.1304e-03,  4.2153e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 1.4345e-03, -2.5559e-03, -8.5724e-04, -2.5221e-03,  1.7710e-03,
         -3.1266e-03,  4.1323e-05, -6.6878e-04,  1.5836e-04,  2.0908e-03,
         -7.4423e-04, -3.7545e-04, -2.3614e-03,  1.4857e-03, -1.4987e-03,
          5.8610e-04,  1.2249e-03,  1.2627e-03,  3.9010e-04, -1.4607e-03,
         -2.3871e-04,  1.6053e-04, -6.6502e-04, -5.0000e-04,  3.2042e-03,
          9.9623e-04,  5.7253e-04,  9.3790e-04],
        [ 6.8411e-04,  9.5200e-04, -9.9285e-04,  3.3383e-03, -8.5224e-04,
          2.1611e-03,  2.0192e-03,  6.7307e-05,  6.5473e-04, -6.4038e-04,
         -2.9787e-03, -2.3678e-03, -7.2530e-04, -2.8311e-03,  1.2308e-03,
          1.2864e-03,  6.6816e-04, -2.2771e-03, -2.5568e-03, -1.2197e-03,
          4.0790e-03, -9.8094e-04, -2.6270e-03, -1.4579e-03, -2.5497e-04,
         -6.4599e-04,  1.6956e-04,  2.7038e-03],
        [ 5.4252e-04,  1.6602e-04,  1.3938e-04,  1.0993e-03, -2.0392e-03,
          2.1465e-04, -1.6477e-03, -2.5406e-04, -2.3514e-04, -7.7118e-04,
         -1.6747e-03, -1.0080e-03,  8.3295e-04, -2.5049e-03,  9.9097e-04,
         -1.2857e-03,  1.0001e-03,  1.8087e-05, -1.1808e-03, -1.0312e-03,
          9.2037e-04, -3.2613e-04,  2.9950e-04, -1.0130e-03, -2.0315e-03,
         -1.3858e-04, -1.4074e-03,  3.3147e-04],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.6764e-03,  1.0787e-03,  1.5668e-03,  1.3307e-03, -3.0035e-03,
          2.6108e-03, -1.0876e-03,  1.4093e-03,  1.8004e-04, -2.2349e-03,
         -4.6559e-04, -7.4600e-04,  2.9334e-03, -2.2447e-03,  3.3826e-03,
         -1.8802e-03, -7.7296e-05,  4.8961e-04, -5.4276e-04,  5.9069e-04,
         -4.9920e-04,  6.4238e-04,  1.2339e-03, -8.5661e-04, -2.4922e-03,
         -4.2440e-04, -1.6357e-03, -1.6140e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-4.5653e-04,  1.3455e-03,  1.3445e-03,  9.1234e-04, -3.2506e-03,
          1.2850e-03, -2.0063e-03, -3.2159e-04, -7.2868e-04, -2.8957e-03,
         -7.4868e-04, -1.1569e-03,  1.7122e-03, -2.0512e-03,  2.6872e-03,
         -1.7845e-03,  9.0860e-05, -1.6262e-04, -8.4018e-04, -6.5263e-04,
         -1.1630e-03,  7.6765e-04,  1.2097e-03,  2.2411e-04, -3.4960e-03,
         -9.7272e-04, -9.2015e-04, -5.9439e-04]])
[grad] final_mlp.layers.0.bias
tensor([ 0.0000e+00,  1.5832e-08,  0.0000e+00, -1.3970e-09,  7.4506e-09,
         1.8626e-09,  0.0000e+00,  2.7940e-09,  0.0000e+00, -9.5461e-09])
loss: [2]: 230.25714111328125
Valition (on train):
5 -> [0]
0 -> [0]
4 -> [0]
1 -> [0]
9 -> [0]
2 -> [0]
1 -> [0]
3 -> [0]
1 -> [0]
4 -> [0]
Accuracy (on train): 0.13
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.92it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 42.15it/s]