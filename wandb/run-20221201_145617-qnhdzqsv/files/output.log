ViT(
  (positional_embedding): PositionalEmbedding2d()
  (encoder_blocks): Sequential(
    (0): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (1): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
    (2): TransformerEncoderBlock(
      (norm1): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((28,), eps=1e-05, elementwise_affine=True)
      (msa): MultiHeadSelfAttention(
        (lin_k): Linear(in_features=28, out_features=28, bias=True)
        (lin_q): Linear(in_features=28, out_features=28, bias=True)
        (lin_v): Linear(in_features=28, out_features=28, bias=True)
      )
      (MLP): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=28, out_features=28, bias=True)
          (1): Linear(in_features=28, out_features=28, bias=True)
        )
      )
    )
  )
  (final_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=28, out_features=10, bias=True)
    )
  )
)
patches_train.shape: (60000, 49, 16)
learning rate: 0.018957345971563982





 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 537/600 [00:11<00:01, 47.04it/s]
loss: [600]: 202.74227905273438
Valition (on train):
5 -> [8] tensor([[1.7059e-02, 4.4073e-11, 1.7389e-01, 2.1193e-01, 1.0565e-04, 1.6832e-01,
         8.3848e-04, 5.1847e-06, 2.2186e-01, 2.0600e-01]],
       grad_fn=<SoftmaxBackward0>)
0 -> [0] tensor([[9.0618e-01, 2.5357e-08, 4.3087e-02, 2.3806e-06, 3.8448e-06, 1.1250e-06,
         3.9982e-02, 6.9263e-08, 1.0744e-02, 1.7893e-06]],
       grad_fn=<SoftmaxBackward0>)
4 -> [4] tensor([[1.0610e-04, 7.0694e-03, 8.4032e-09, 1.3711e-08, 9.9241e-01, 3.6753e-11,
         2.0462e-06, 5.2847e-07, 9.0239e-06, 4.0257e-04]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[1.3313e-06, 1.0000e+00, 2.4176e-13, 2.9288e-14, 3.2845e-06, 7.5261e-17,
         1.6140e-10, 1.7703e-09, 2.3574e-10, 7.6470e-09]],
       grad_fn=<SoftmaxBackward0>)
9 -> [4] tensor([[5.6437e-05, 2.9694e-06, 3.7473e-08, 1.7292e-09, 9.9984e-01, 1.2565e-11,
         3.3496e-05, 3.5314e-08, 3.1736e-05, 3.2058e-05]],
       grad_fn=<SoftmaxBackward0>)
2 -> [5] tensor([[5.5650e-03, 5.8418e-09, 8.6897e-02, 9.9558e-02, 5.6605e-04, 4.8263e-01,
         8.9037e-03, 8.4291e-04, 1.1342e-05, 3.1503e-01]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[2.7854e-06, 9.9999e-01, 1.2601e-13, 6.9054e-13, 5.0315e-06, 1.7197e-18,
         1.4437e-12, 1.4807e-09, 6.8998e-10, 1.5196e-12]],
       grad_fn=<SoftmaxBackward0>)
3 -> [0] tensor([[9.9913e-01, 7.3808e-06, 3.2606e-06, 1.3797e-05, 1.6922e-04, 1.1564e-09,
         6.5166e-08, 1.8051e-08, 6.6317e-04, 8.7727e-06]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[6.2181e-12, 1.0000e+00, 1.0540e-17, 3.9144e-17, 3.5551e-08, 9.1427e-21,
         1.6393e-14, 2.0103e-09, 6.9496e-15, 1.8512e-11]],
       grad_fn=<SoftmaxBackward0>)
4 -> [4] tensor([[9.4647e-08, 3.6037e-04, 2.5074e-08, 1.0052e-10, 9.9796e-01, 9.0328e-11,
         1.6367e-03, 1.2428e-06, 2.8676e-08, 3.9711e-05]],
       grad_fn=<SoftmaxBackward0>)
Accuracy (on train): 0.44
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:12<00:00, 46.64it/s]






 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 594/600 [00:12<00:00, 41.96it/s]
loss: [1200]: 193.96682739257812
Valition (on train):
5 -> [8] tensor([[8.7610e-07, 1.1028e-14, 2.8812e-09, 9.8234e-06, 1.1478e-11, 5.7220e-06,
         1.9784e-13, 1.5395e-08, 9.9998e-01, 3.1582e-08]],
       grad_fn=<SoftmaxBackward0>)
0 -> [8] tensor([[2.4177e-02, 1.3360e-10, 3.6872e-06, 3.5553e-09, 1.0148e-09, 8.1396e-13,
         1.7859e-07, 1.1556e-13, 9.7582e-01, 1.3679e-12]],
       grad_fn=<SoftmaxBackward0>)
4 -> [4] tensor([[8.8264e-08, 6.0406e-04, 8.5116e-15, 3.3499e-10, 9.8638e-01, 8.7064e-10,
         4.8212e-08, 2.4200e-09, 1.2821e-02, 1.9549e-04]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[2.0946e-08, 1.0000e+00, 2.3432e-17, 2.6441e-16, 3.7674e-10, 8.1366e-22,
         7.1593e-15, 2.7495e-12, 6.0732e-10, 1.8298e-15]],
       grad_fn=<SoftmaxBackward0>)
9 -> [4] tensor([[6.6647e-05, 2.6422e-05, 8.9001e-08, 5.2659e-09, 8.9827e-01, 8.8935e-08,
         5.7837e-03, 1.3650e-07, 2.6108e-03, 9.3238e-02]],
       grad_fn=<SoftmaxBackward0>)
2 -> [5] tensor([[2.3236e-09, 1.0114e-09, 8.1882e-08, 6.8699e-04, 1.1839e-01, 5.2489e-01,
         3.5559e-02, 6.5150e-06, 3.6443e-05, 3.2043e-01]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[1.0909e-03, 9.9891e-01, 7.3665e-13, 1.1321e-11, 3.8346e-08, 2.8102e-18,
         5.6746e-15, 1.0096e-06, 7.4780e-07, 5.7764e-12]],
       grad_fn=<SoftmaxBackward0>)
3 -> [8] tensor([[1.2450e-02, 2.4676e-13, 2.0577e-11, 2.2941e-06, 2.1393e-14, 7.5258e-13,
         1.2600e-18, 1.9764e-14, 9.8755e-01, 7.6720e-17]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[4.0921e-15, 1.0000e+00, 4.1373e-24, 5.7862e-21, 1.0153e-08, 9.8290e-26,
         5.2103e-17, 7.4888e-14, 7.0097e-17, 4.5731e-15]],
       grad_fn=<SoftmaxBackward0>)
4 -> [4] tensor([[2.3061e-14, 1.3821e-05, 2.1126e-17, 1.1257e-15, 9.9998e-01, 4.4008e-13,
         1.4766e-08, 4.9465e-12, 2.6428e-11, 2.2949e-06]],
       grad_fn=<SoftmaxBackward0>)
Accuracy (on train): 0.51
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:12<00:00, 46.42it/s]






100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 599/600 [00:13<00:00, 37.10it/s]
loss: [1800]: 205.70672607421875
Valition (on train):
5 -> [2] tensor([[4.2616e-02, 9.3723e-16, 9.4989e-01, 1.8072e-04, 2.0655e-17, 5.7389e-16,
         2.4474e-08, 1.5088e-21, 7.3147e-03, 2.5239e-21]],
       grad_fn=<SoftmaxBackward0>)
0 -> [0] tensor([[9.9942e-01, 5.0339e-16, 4.8830e-04, 8.7148e-05, 1.1939e-31, 1.0562e-22,
         1.3150e-22, 1.8649e-14, 1.9968e-09, 3.3981e-27]],
       grad_fn=<SoftmaxBackward0>)
4 -> [1] tensor([[5.3699e-23, 1.0000e+00, 1.3632e-36, 9.1534e-35, 5.8662e-12, 6.1342e-26,
         1.4733e-28, 2.6927e-16, 7.2108e-15, 4.4149e-17]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[6.4808e-05, 9.8971e-01, 3.6356e-14, 4.5589e-12, 1.2071e-13, 1.0010e-14,
         6.3579e-21, 1.7617e-05, 1.0206e-02, 7.2406e-12]],
       grad_fn=<SoftmaxBackward0>)
9 -> [1] tensor([[9.1639e-07, 6.0075e-01, 4.5209e-15, 1.9050e-19, 3.9692e-01, 3.3052e-12,
         1.0922e-14, 8.1004e-06, 8.7068e-04, 1.4540e-03]],
       grad_fn=<SoftmaxBackward0>)
2 -> [3] tensor([[8.2697e-11, 6.5163e-22, 6.2681e-05, 9.9983e-01, 5.5894e-08, 2.7212e-06,
         9.9474e-05, 6.5397e-17, 2.5898e-07, 9.8998e-13]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[2.0582e-08, 1.0000e+00, 1.3007e-16, 1.7118e-18, 7.8461e-16, 1.0509e-24,
         9.7534e-23, 4.0625e-08, 3.1347e-11, 4.9374e-16]],
       grad_fn=<SoftmaxBackward0>)
3 -> [0] tensor([[9.9914e-01, 8.7680e-23, 3.1339e-12, 8.5977e-04, 1.9662e-34, 1.0331e-23,
         4.1705e-28, 9.3045e-27, 4.4032e-11, 4.4259e-33]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[8.9735e-18, 1.0000e+00, 6.6342e-27, 1.1527e-25, 1.3187e-13, 1.1150e-26,
         7.2630e-26, 2.8056e-08, 7.0097e-16, 4.7594e-14]],
       grad_fn=<SoftmaxBackward0>)
4 -> [7] tensor([[1.8363e-21, 1.6202e-02, 4.2997e-18, 1.6078e-16, 1.5244e-01, 6.4464e-12,
         4.3325e-11, 8.3104e-01, 2.4893e-12, 3.1809e-04]],
       grad_fn=<SoftmaxBackward0>)
Accuracy (on train): 0.39
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:13<00:00, 42.97it/s]





 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 524/600 [00:11<00:01, 46.49it/s]
loss: [2400]: 220.40753173828125
Valition (on train):
5 -> [9] tensor([[7.8994e-10, 1.4581e-40, 4.9032e-11, 2.4771e-18, 7.0459e-25, 9.1032e-09,
         3.6086e-24, 3.0249e-13, 6.9185e-06, 9.9999e-01]],
       grad_fn=<SoftmaxBackward0>)
0 -> [1] tensor([[5.9365e-06, 9.9999e-01, 1.4727e-17, 1.9966e-13, 1.6220e-14, 9.9115e-17,
         4.2133e-12, 1.8364e-09, 8.4898e-09, 1.9531e-13]],
       grad_fn=<SoftmaxBackward0>)
4 -> [9] tensor([[1.3420e-34, 2.2787e-25, 1.8081e-41, 9.5135e-23, 2.1618e-18, 1.9592e-22,
         1.5695e-43, 1.8654e-14, 5.9546e-20, 1.0000e+00]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[8.7079e-41, 1.0000e+00, 0.0000e+00, 5.0161e-41, 2.2619e-23, 1.5028e-38,
         1.4181e-42, 3.4859e-19, 4.9624e-24, 1.7375e-22]],
       grad_fn=<SoftmaxBackward0>)
9 -> [9] tensor([[7.8054e-27, 3.5402e-23, 1.0533e-19, 4.9906e-23, 1.3103e-16, 4.8872e-14,
         1.2782e-33, 1.7604e-04, 1.0650e-05, 9.9981e-01]],
       grad_fn=<SoftmaxBackward0>)
2 -> [9] tensor([[2.7460e-15, 7.7959e-24, 1.5688e-14, 1.5747e-05, 1.9373e-11, 2.9165e-05,
         2.5037e-23, 2.6658e-08, 3.5171e-01, 6.4825e-01]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 2.9387e-29, 0.0000e+00,
         0.0000e+00, 1.2689e-20, 1.2439e-36, 3.3384e-34]],
       grad_fn=<SoftmaxBackward0>)
3 -> [3] tensor([[7.2754e-07, 3.4855e-26, 1.6419e-18, 7.4481e-01, 1.7786e-30, 1.7626e-18,
         1.7605e-38, 3.8015e-27, 2.5518e-01, 1.0559e-26]],
       grad_fn=<SoftmaxBackward0>)
1 -> [1] tensor([[0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 6.5753e-24, 0.0000e+00,
         2.6032e-41, 8.3380e-24, 1.6816e-44, 2.8218e-29]],
       grad_fn=<SoftmaxBackward0>)
4 -> [9] tensor([[3.1666e-27, 2.6126e-23, 5.3405e-29, 1.2020e-27, 2.7028e-10, 1.8595e-19,
         9.2400e-29, 2.8034e-18, 1.3043e-07, 1.0000e+00]],
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:13<00:00, 43.73it/s]
  1%|██▎                                                                                                                                                                              | 8/600 [00:00<00:16, 35.72it/s]
Accuracy (on train): 0.27






 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                          | 509/600 [00:11<00:02, 43.72it/s]
loss: [3000]: 237.11500549316406
Valition (on train):
5 -> [7] tensor([[6.3304e-37, 3.6218e-25, 1.1137e-33, 9.6429e-19, 9.8107e-21, 3.8197e-27,
         3.7556e-34, 1.0000e+00, 3.0033e-20, 1.3938e-24]],
       grad_fn=<SoftmaxBackward0>)
0 -> [7] tensor([[0.0000e+00, 7.3989e-43, 0.0000e+00, 2.4119e-31, 1.3750e-28, 3.2090e-43,
         0.0000e+00, 1.0000e+00, 2.5907e-39, 1.4998e-32]],
       grad_fn=<SoftmaxBackward0>)
4 -> [7] tensor([[0.0000e+00, 2.0025e-17, 0.0000e+00, 1.3267e-34, 4.0427e-30, 0.0000e+00,
         0.0000e+00, 1.0000e+00, 1.5751e-42, 2.4103e-26]],
       grad_fn=<SoftmaxBackward0>)
1 -> [7] tensor([[0.0000e+00, 1.5360e-22, 2.5223e-44, 2.6776e-26, 1.0221e-20, 2.4414e-36,
         5.9353e-41, 1.0000e+00, 2.1043e-29, 4.6064e-24]],
       grad_fn=<SoftmaxBackward0>)
9 -> [7] tensor([[0.0000e+00, 4.2046e-33, 0.0000e+00, 6.7175e-31, 1.4325e-19, 1.3228e-42,
         3.3183e-42, 1.0000e+00, 5.5569e-39, 4.4068e-25]],
       grad_fn=<SoftmaxBackward0>)
2 -> [7] tensor([[4.6935e-41, 4.5700e-23, 2.4999e-42, 3.3466e-26, 9.3044e-22, 2.0879e-38,
         1.1013e-34, 1.0000e+00, 6.0584e-37, 6.0087e-23]],
       grad_fn=<SoftmaxBackward0>)
1 -> [7] tensor([[0.0000e+00, 1.6538e-29, 0.0000e+00, 2.2705e-35, 2.4624e-34, 0.0000e+00,
         0.0000e+00, 1.0000e+00, 2.9381e-35, 3.3472e-34]],
       grad_fn=<SoftmaxBackward0>)
3 -> [7] tensor([[1.9454e-40, 2.7276e-16, 7.6361e-41, 4.5788e-22, 5.0678e-17, 1.1745e-34,
         1.4057e-36, 1.0000e+00, 4.2252e-25, 5.3508e-22]],
       grad_fn=<SoftmaxBackward0>)
1 -> [7] tensor([[0.0000e+00, 4.4226e-24, 0.0000e+00, 1.4830e-39, 4.1869e-30, 0.0000e+00,
         0.0000e+00, 1.0000e+00, 0.0000e+00, 4.1439e-28]],
       grad_fn=<SoftmaxBackward0>)
4 -> [7] tensor([[0.0000e+00, 2.5877e-23, 0.0000e+00, 1.5631e-28, 5.8800e-20, 7.7894e-40,
         1.1336e-37, 1.0000e+00, 1.8989e-37, 1.6902e-22]],
       grad_fn=<SoftmaxBackward0>)
Accuracy (on train): 0.1

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:13<00:00, 43.30it/s]





 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 559/600 [00:11<00:00, 46.87it/s]
loss: [3600]: 237.11500549316406
Valition (on train):
5 -> [7] tensor([[8.5958e-31, 3.6355e-21, 1.8626e-28, 9.8728e-16, 4.7032e-17, 2.8116e-23,
         3.9132e-28, 1.0000e+00, 6.1005e-18, 2.4469e-20]],
       grad_fn=<SoftmaxBackward0>)
0 -> [7] tensor([[0.0000e+00, 3.3178e-35, 1.6157e-42, 1.3061e-25, 6.9945e-23, 7.8150e-36,
         1.2992e-40, 1.0000e+00, 1.8967e-32, 1.4243e-26]],
       grad_fn=<SoftmaxBackward0>)
4 -> [7] tensor([[1.4013e-45, 5.6158e-15, 0.0000e+00, 1.3054e-28, 7.4092e-25, 4.8904e-40,
         0.0000e+00, 1.0000e+00, 2.0352e-35, 4.6738e-22]],
       grad_fn=<SoftmaxBackward0>)
1 -> [7] tensor([[1.7804e-38, 4.6578e-19, 3.0120e-37, 9.6614e-22, 4.8899e-17, 1.4332e-30,
         4.2302e-34, 1.0000e+00, 6.9783e-25, 4.6376e-20]],
       grad_fn=<SoftmaxBackward0>)
9 -> [7] tensor([[0.0000e+00, 1.2434e-26, 2.1720e-43, 4.2273e-25, 1.5697e-15, 2.2102e-35,
         1.1533e-34, 1.0000e+00, 3.3210e-32, 1.3193e-20]],
       grad_fn=<SoftmaxBackward0>)
2 -> [7] tensor([[3.6881e-34, 1.7319e-19, 3.2530e-35, 1.6808e-21, 6.0848e-18, 5.2399e-32,
         8.3921e-29, 1.0000e+00, 1.1321e-30, 3.3113e-19]],
       grad_fn=<SoftmaxBackward0>)
1 -> [7] tensor([[0.0000e+00, 1.7801e-23, 0.0000e+00, 2.3787e-29, 6.4077e-29, 3.7989e-42,
         0.0000e+00, 1.0000e+00, 2.4264e-30, 2.6142e-28]],
       grad_fn=<SoftmaxBackward0>)
3 -> [7] tensor([[3.4980e-34, 3.3878e-14, 1.7094e-34, 1.7881e-18, 2.8343e-14, 2.7291e-29,
         9.4757e-31, 1.0000e+00, 1.4850e-21, 1.5186e-18]],
       grad_fn=<SoftmaxBackward0>)
1 -> [7] tensor([[0.0000e+00, 2.3972e-20, 0.0000e+00, 1.2918e-32, 6.4441e-25, 1.2612e-44,
         0.0000e+00, 1.0000e+00, 9.0084e-39, 1.5586e-23]],
       grad_fn=<SoftmaxBackward0>)
4 -> [7] tensor([[6.9381e-40, 2.1507e-19, 2.9302e-39, 1.6318e-23, 1.4870e-16, 2.2649e-33,
         9.6924e-32, 1.0000e+00, 4.3007e-31, 6.2032e-19]],
       grad_fn=<SoftmaxBackward0>)
Accuracy (on train): 0.1
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:12<00:00, 46.72it/s]

 25%|████████████████████████████████████████████▎                                                                                                                                  | 152/600 [00:03<00:09, 47.25it/s]
Traceback (most recent call last):
  File "/Users/tdeegan/dev/transformers/main.py", line 124, in <module>
    y_pred = model(x)
  File "/Users/tdeegan/dev/transformers/vit.py", line 119, in __call__
    x = self.encoder_blocks(x)
  File "/Users/tdeegan/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/tdeegan/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/Users/tdeegan/dev/transformers/vit.py", line 45, in __call__
    x = self.msa(self.norm1(inputs)) + inputs
  File "/Users/tdeegan/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/Users/tdeegan/Library/Python/3.10/lib/python/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/Users/tdeegan/Library/Python/3.10/lib/python/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
KeyboardInterrupt